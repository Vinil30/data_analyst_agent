{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VlKBCsKGwSc",
        "outputId": "1a36dc3d-0634-4172-8064-ea5c8b5ffbdd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.0)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " pip install pdfplumber pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-w6FabJG7eD",
        "outputId": "10c16dad-c102-4f56-b7e3-4954a9711f7d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytesseract, pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.1 pytesseract-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "fgolJ6IlGkSF",
        "outputId": "d4bdf6aa-15af-4557-c906-019c3507885d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://1ba80c650b1b78a913.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1ba80c650b1b78a913.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Import all required libraries\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "import base64\n",
        "from io import BytesIO\n",
        "import pandas as pd\n",
        "import docx\n",
        "import pdfplumber\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import torch\n",
        "from openai import OpenAI\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from IPython.display import display, Markdown\n",
        "import gradio as gr\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Together.ai API setup\n",
        "load_dotenv()\n",
        "MODEL_NAME = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\"\n",
        "\n",
        "# Initialize Together client\n",
        "client = OpenAI(\n",
        "    api_key=os.getenv(\"TOGETHER_API_KEY\"),\n",
        "    base_url=\"https://api.together.xyz/v1\",\n",
        ")\n",
        "\n",
        "# File reading functions\n",
        "def read_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    return '\\n'.join([p.text for p in doc.paragraphs])\n",
        "\n",
        "def read_pdf(file_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "    return text\n",
        "\n",
        "def read_image(file_path):\n",
        "    return pytesseract.image_to_string(Image.open(file_path))\n",
        "\n",
        "def read_txt(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return f.read()\n",
        "\n",
        "def extract_data_from_file(file_path):\n",
        "    _, ext = os.path.splitext(file_path)\n",
        "    ext = ext.lower()\n",
        "\n",
        "    try:\n",
        "        if ext == '.csv':\n",
        "            return {'type': 'dataframe', 'data': pd.read_csv(file_path)}\n",
        "        elif ext == '.xlsx':\n",
        "            return {'type': 'dataframe', 'data': pd.read_excel(file_path)}\n",
        "        elif ext == '.txt':\n",
        "            return {'type': 'text', 'data': read_txt(file_path)}\n",
        "        elif ext == '.pdf':\n",
        "            return {'type': 'text', 'data': read_pdf(file_path)}\n",
        "        elif ext == '.docx':\n",
        "            return {'type': 'text', 'data': read_docx(file_path)}\n",
        "        elif ext in ['.jpg', '.jpeg', '.png']:\n",
        "            return {'type': 'text', 'data': read_image(file_path)}\n",
        "        else:\n",
        "            return {'type': 'error', 'data': f\"Unsupported file type: {ext}\"}\n",
        "    except Exception as e:\n",
        "        return {'type': 'error', 'data': f\"Error reading file: {str(e)}\"}\n",
        "\n",
        "# System message setup\n",
        "system_message = \"\"\"You are an expert data analyst assistant which analyses data,\n",
        "provides answers to questions and generates visualizations. You will be given text\n",
        "data extracted from files. Analyze the data and answer user questions.\n",
        "Follow these rules:\n",
        "1. Be concise but precise\n",
        "2. For tabular data, always show sample rows\n",
        "3. Explain technical terms\n",
        "4. Suggest follow-up questions\"\"\"\n",
        "\n",
        "# Visualization function\n",
        "def generate_visualization(data_description):\n",
        "    response = client.images.generate(\n",
        "        model=\"stability-ai/sdxl\",\n",
        "        prompt=f\"Create an accurate data visualization showing: {data_description}\",\n",
        "        size=\"1024x1024\",\n",
        "        quality=\"standard\",\n",
        "        n=1\n",
        "    )\n",
        "    return response.data[0].url\n",
        "\n",
        "# Chat function\n",
        "def chat_with_agent(messages, history=None):\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    full_messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "\n",
        "    for human, assistant in history:\n",
        "        full_messages.append({\"role\": \"user\", \"content\": human})\n",
        "        full_messages.append({\"role\": \"assistant\", \"content\": assistant})\n",
        "\n",
        "    full_messages.extend(messages)\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=full_messages,\n",
        "        temperature=0.7,\n",
        "        max_tokens=2000\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Gradio UI setup\n",
        "def analyze_data(file, question):\n",
        "    # Extract data from file\n",
        "    data = extract_data_from_file(file.name)\n",
        "\n",
        "    if data['type'] == 'error':\n",
        "        return data['data']\n",
        "\n",
        "    if data['type'] == 'dataframe':\n",
        "        data_str = data['data'].head().to_string()\n",
        "    else:\n",
        "        data_str = data['data']\n",
        "\n",
        "    # Ask the question\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": f\"Data:\\n{data_str}\\n\\nQuestion: {question}\"}\n",
        "    ]\n",
        "\n",
        "    answer = chat_with_agent(messages)\n",
        "\n",
        "    # Check if visualization is needed\n",
        "    if \"visualization\" in question.lower() or \"graph\" in question.lower():\n",
        "        viz_url = generate_visualization(f\"Data: {data_str}\\nRequest: {question}\")\n",
        "        return f\"{answer}\\n\\nVisualization: {viz_url}\"\n",
        "    else:\n",
        "        return answer\n",
        "\n",
        "# Create Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=analyze_data,\n",
        "    inputs=[\n",
        "        gr.File(label=\"Upload your file\"),\n",
        "        gr.Textbox(label=\"Your question about the data\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Analysis results\"),\n",
        "    title=\"Data Analyst Agent\",\n",
        "    description=\"Upload a file (CSV, Excel, PDF, etc.) and ask questions about the data.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch()"
      ]
    }
  ]
}